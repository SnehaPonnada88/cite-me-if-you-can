# ğŸ§  Cite Me If You Can

A mini data science challenge that combines **semantic search** + **LLM-powered Q&A** on academic research chunks.  
Built using FastAPI, Qdrant vector database, Sentence Transformers, and OpenAI GPT.

---

## ğŸš€ What It Does

> Give me context, and Iâ€™ll cite it back.

This system enables two main features:

1. **Semantic Similarity Search**  
   Search through academic chunks using vector embeddings and retrieve the most relevant ones.

2. **GPT-powered Contextual Answering**  
   Ask research questions, and receive answers generated by OpenAI GPT with real citations from source documents.

---


## ğŸ§ª API Endpoints

> Testable via Swagger UI at [`/docs`](http://127.0.0.1:8000/docs)


## Tech Stack

ğŸ§  OpenAI GPT-3.5 Turbo

ğŸ” Qdrant vector search

ğŸ§¾ Sentence Transformers (all-MiniLM-L6-v2)

âš¡ FastAPI

ğŸ³ Docker (for Qdrant container)

## ğŸ› ï¸ Setup & Run
bash

# 1. Create virtual env
conda create -n cite-ai python=3.10
conda activate cite-ai

# 2. Install dependencies
pip install -r requirements.txt

# 3. Run Qdrant in Docker
docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant

# 4. Run the FastAPI app
uvicorn main:app --reload

# 5. Open the Swagger UI
http://127.0.0.1:8000/docs

## 'ğŸ“¦ Versioning'

v0.1 â€“ Initial folders setup
v0.2 â€“ Ingestion pipeline setupwith embedding and Qdrant Integration
v0.3 â€“ Similarity Search Endpoint
v0.4 â€“ Dockerized Qdrant integration
v0.5 - GPT Answer API Endpoint
v0.6 - Upload Endpoint
v0.7 â€“ Usage count for endpoint tracking



